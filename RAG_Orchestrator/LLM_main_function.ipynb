{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rBi4XNKKyESx",
        "outputId": "3f869c35-4f3a-4336-e87d-e55c57ab5159"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: langgraph in c:\\program files\\python311\\lib\\site-packages (0.4.8)\n",
            "Requirement already satisfied: langsmith in c:\\program files\\python311\\lib\\site-packages (0.3.45)\n",
            "Requirement already satisfied: langchain in c:\\program files\\python311\\lib\\site-packages (0.3.25)\n",
            "Requirement already satisfied: langchain_community in c:\\program files\\python311\\lib\\site-packages (0.3.25)\n",
            "Requirement already satisfied: langchain-core>=0.1 in c:\\program files\\python311\\lib\\site-packages (from langgraph) (0.3.65)\n",
            "Requirement already satisfied: langgraph-checkpoint>=2.0.26 in c:\\program files\\python311\\lib\\site-packages (from langgraph) (2.0.26)\n",
            "Requirement already satisfied: langgraph-prebuilt>=0.2.0 in c:\\program files\\python311\\lib\\site-packages (from langgraph) (0.2.2)\n",
            "Requirement already satisfied: langgraph-sdk>=0.1.42 in c:\\program files\\python311\\lib\\site-packages (from langgraph) (0.1.70)\n",
            "Requirement already satisfied: pydantic>=2.7.4 in c:\\program files\\python311\\lib\\site-packages (from langgraph) (2.11.5)\n",
            "Requirement already satisfied: xxhash>=3.5.0 in c:\\program files\\python311\\lib\\site-packages (from langgraph) (3.5.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\program files\\python311\\lib\\site-packages (from langsmith) (0.28.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in c:\\program files\\python311\\lib\\site-packages (from langsmith) (3.10.18)\n",
            "Requirement already satisfied: packaging>=23.2 in c:\\program files\\python311\\lib\\site-packages (from langsmith) (24.2)\n",
            "Requirement already satisfied: requests<3,>=2 in c:\\program files\\python311\\lib\\site-packages (from langsmith) (2.32.4)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in c:\\program files\\python311\\lib\\site-packages (from langsmith) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in c:\\program files\\python311\\lib\\site-packages (from langsmith) (0.23.0)\n",
            "Requirement already satisfied: anyio in c:\\program files\\python311\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith) (4.9.0)\n",
            "Requirement already satisfied: certifi in c:\\program files\\python311\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith) (2022.12.7)\n",
            "Requirement already satisfied: httpcore==1.* in c:\\program files\\python311\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith) (1.0.9)\n",
            "Requirement already satisfied: idna in c:\\program files\\python311\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith) (3.4)\n",
            "Requirement already satisfied: h11>=0.16 in c:\\program files\\python311\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith) (0.16.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in c:\\program files\\python311\\lib\\site-packages (from pydantic>=2.7.4->langgraph) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in c:\\program files\\python311\\lib\\site-packages (from pydantic>=2.7.4->langgraph) (2.33.2)\n",
            "Requirement already satisfied: typing-extensions>=4.12.2 in c:\\program files\\python311\\lib\\site-packages (from pydantic>=2.7.4->langgraph) (4.13.0)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in c:\\program files\\python311\\lib\\site-packages (from pydantic>=2.7.4->langgraph) (0.4.1)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\program files\\python311\\lib\\site-packages (from requests<3,>=2->langsmith) (3.1.0)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\program files\\python311\\lib\\site-packages (from requests<3,>=2->langsmith) (1.26.15)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.8 in c:\\program files\\python311\\lib\\site-packages (from langchain) (0.3.8)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in c:\\program files\\python311\\lib\\site-packages (from langchain) (2.0.40)\n",
            "Requirement already satisfied: PyYAML>=5.3 in c:\\program files\\python311\\lib\\site-packages (from langchain) (6.0.2)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in c:\\program files\\python311\\lib\\site-packages (from langchain-core>=0.1->langgraph) (9.1.2)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in c:\\program files\\python311\\lib\\site-packages (from langchain-core>=0.1->langgraph) (1.33)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in c:\\program files\\python311\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core>=0.1->langgraph) (3.0.0)\n",
            "Requirement already satisfied: greenlet>=1 in c:\\program files\\python311\\lib\\site-packages (from SQLAlchemy<3,>=1.4->langchain) (3.1.1)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in c:\\program files\\python311\\lib\\site-packages (from langchain_community) (3.12.12)\n",
            "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in c:\\program files\\python311\\lib\\site-packages (from langchain_community) (0.6.7)\n",
            "Requirement already satisfied: pydantic-settings<3.0.0,>=2.4.0 in c:\\program files\\python311\\lib\\site-packages (from langchain_community) (2.9.1)\n",
            "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in c:\\program files\\python311\\lib\\site-packages (from langchain_community) (0.4.0)\n",
            "Requirement already satisfied: numpy>=1.26.2 in c:\\program files\\python311\\lib\\site-packages (from langchain_community) (1.26.4)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in c:\\program files\\python311\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in c:\\program files\\python311\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in c:\\program files\\python311\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in c:\\program files\\python311\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\program files\\python311\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (6.4.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in c:\\program files\\python311\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in c:\\program files\\python311\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.20.1)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in c:\\program files\\python311\\lib\\site-packages (from dataclasses-json<0.7,>=0.5.7->langchain_community) (3.26.1)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in c:\\program files\\python311\\lib\\site-packages (from dataclasses-json<0.7,>=0.5.7->langchain_community) (0.9.0)\n",
            "Requirement already satisfied: python-dotenv>=0.21.0 in c:\\program files\\python311\\lib\\site-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain_community) (1.1.0)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in c:\\program files\\python311\\lib\\site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain_community) (1.1.0)\n",
            "Requirement already satisfied: ormsgpack<2.0.0,>=1.8.0 in c:\\program files\\python311\\lib\\site-packages (from langgraph-checkpoint>=2.0.26->langgraph) (1.10.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in c:\\program files\\python311\\lib\\site-packages (from anyio->httpx<1,>=0.23.0->langsmith) (1.3.1)\n",
            "Requirement already satisfied: faiss-cpu in c:\\program files\\python311\\lib\\site-packages (1.11.0)\n",
            "Requirement already satisfied: numpy<3.0,>=1.25.0 in c:\\program files\\python311\\lib\\site-packages (from faiss-cpu) (1.26.4)\n",
            "Requirement already satisfied: packaging in c:\\program files\\python311\\lib\\site-packages (from faiss-cpu) (24.2)\n",
            "Requirement already satisfied: sentence-transformers in c:\\program files\\python311\\lib\\site-packages (4.1.0)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in c:\\program files\\python311\\lib\\site-packages (from sentence-transformers) (4.52.4)\n",
            "Requirement already satisfied: tqdm in c:\\program files\\python311\\lib\\site-packages (from sentence-transformers) (4.67.1)\n",
            "Requirement already satisfied: torch>=1.11.0 in c:\\program files\\python311\\lib\\site-packages (from sentence-transformers) (2.7.0+cu128)\n",
            "Requirement already satisfied: scikit-learn in c:\\program files\\python311\\lib\\site-packages (from sentence-transformers) (1.2.2)\n",
            "Requirement already satisfied: scipy in c:\\program files\\python311\\lib\\site-packages (from sentence-transformers) (1.10.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.20.0 in c:\\program files\\python311\\lib\\site-packages (from sentence-transformers) (0.33.0)\n",
            "Requirement already satisfied: Pillow in c:\\program files\\python311\\lib\\site-packages (from sentence-transformers) (9.5.0)\n",
            "Requirement already satisfied: typing_extensions>=4.5.0 in c:\\program files\\python311\\lib\\site-packages (from sentence-transformers) (4.13.0)\n",
            "Requirement already satisfied: filelock in c:\\program files\\python311\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (3.18.0)\n",
            "Requirement already satisfied: numpy>=1.17 in c:\\program files\\python311\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in c:\\program files\\python311\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in c:\\program files\\python311\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in c:\\program files\\python311\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in c:\\program files\\python311\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2.32.4)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\program files\\python311\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in c:\\program files\\python311\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.5.3)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in c:\\program files\\python311\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2025.3.2)\n",
            "Requirement already satisfied: sympy>=1.13.3 in c:\\program files\\python311\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (1.14.0)\n",
            "Requirement already satisfied: networkx in c:\\program files\\python311\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in c:\\program files\\python311\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (3.1.6)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\program files\\python311\\lib\\site-packages (from sympy>=1.13.3->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
            "Requirement already satisfied: colorama in c:\\program files\\python311\\lib\\site-packages (from tqdm->sentence-transformers) (0.4.6)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in c:\\program files\\python311\\lib\\site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (2.1.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\program files\\python311\\lib\\site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence-transformers) (3.1.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\program files\\python311\\lib\\site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence-transformers) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\program files\\python311\\lib\\site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence-transformers) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\program files\\python311\\lib\\site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence-transformers) (2022.12.7)\n",
            "Requirement already satisfied: joblib>=1.1.1 in c:\\program files\\python311\\lib\\site-packages (from scikit-learn->sentence-transformers) (1.2.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\program files\\python311\\lib\\site-packages (from scikit-learn->sentence-transformers) (3.1.0)\n",
            "Requirement already satisfied: langchain-huggingface in c:\\program files\\python311\\lib\\site-packages (0.3.0)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.65 in c:\\program files\\python311\\lib\\site-packages (from langchain-huggingface) (0.3.65)\n",
            "Requirement already satisfied: tokenizers>=0.19.1 in c:\\program files\\python311\\lib\\site-packages (from langchain-huggingface) (0.21.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.30.2 in c:\\program files\\python311\\lib\\site-packages (from langchain-huggingface) (0.33.0)\n",
            "Requirement already satisfied: langsmith<0.4,>=0.3.45 in c:\\program files\\python311\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.65->langchain-huggingface) (0.3.45)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in c:\\program files\\python311\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.65->langchain-huggingface) (9.1.2)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in c:\\program files\\python311\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.65->langchain-huggingface) (1.33)\n",
            "Requirement already satisfied: PyYAML>=5.3 in c:\\program files\\python311\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.65->langchain-huggingface) (6.0.2)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in c:\\program files\\python311\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.65->langchain-huggingface) (24.2)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in c:\\program files\\python311\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.65->langchain-huggingface) (4.13.0)\n",
            "Requirement already satisfied: pydantic>=2.7.4 in c:\\program files\\python311\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.65->langchain-huggingface) (2.11.5)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in c:\\program files\\python311\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.65->langchain-huggingface) (3.0.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\program files\\python311\\lib\\site-packages (from langsmith<0.4,>=0.3.45->langchain-core<1.0.0,>=0.3.65->langchain-huggingface) (0.28.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in c:\\program files\\python311\\lib\\site-packages (from langsmith<0.4,>=0.3.45->langchain-core<1.0.0,>=0.3.65->langchain-huggingface) (3.10.18)\n",
            "Requirement already satisfied: requests<3,>=2 in c:\\program files\\python311\\lib\\site-packages (from langsmith<0.4,>=0.3.45->langchain-core<1.0.0,>=0.3.65->langchain-huggingface) (2.32.4)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in c:\\program files\\python311\\lib\\site-packages (from langsmith<0.4,>=0.3.45->langchain-core<1.0.0,>=0.3.65->langchain-huggingface) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in c:\\program files\\python311\\lib\\site-packages (from langsmith<0.4,>=0.3.45->langchain-core<1.0.0,>=0.3.65->langchain-huggingface) (0.23.0)\n",
            "Requirement already satisfied: anyio in c:\\program files\\python311\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.3.45->langchain-core<1.0.0,>=0.3.65->langchain-huggingface) (4.9.0)\n",
            "Requirement already satisfied: certifi in c:\\program files\\python311\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.3.45->langchain-core<1.0.0,>=0.3.65->langchain-huggingface) (2022.12.7)\n",
            "Requirement already satisfied: httpcore==1.* in c:\\program files\\python311\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.3.45->langchain-core<1.0.0,>=0.3.65->langchain-huggingface) (1.0.9)\n",
            "Requirement already satisfied: idna in c:\\program files\\python311\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.3.45->langchain-core<1.0.0,>=0.3.65->langchain-huggingface) (3.4)\n",
            "Requirement already satisfied: h11>=0.16 in c:\\program files\\python311\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.3.45->langchain-core<1.0.0,>=0.3.65->langchain-huggingface) (0.16.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in c:\\program files\\python311\\lib\\site-packages (from pydantic>=2.7.4->langchain-core<1.0.0,>=0.3.65->langchain-huggingface) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in c:\\program files\\python311\\lib\\site-packages (from pydantic>=2.7.4->langchain-core<1.0.0,>=0.3.65->langchain-huggingface) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in c:\\program files\\python311\\lib\\site-packages (from pydantic>=2.7.4->langchain-core<1.0.0,>=0.3.65->langchain-huggingface) (0.4.1)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\program files\\python311\\lib\\site-packages (from requests<3,>=2->langsmith<0.4,>=0.3.45->langchain-core<1.0.0,>=0.3.65->langchain-huggingface) (3.1.0)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\program files\\python311\\lib\\site-packages (from requests<3,>=2->langsmith<0.4,>=0.3.45->langchain-core<1.0.0,>=0.3.65->langchain-huggingface) (1.26.15)\n",
            "Requirement already satisfied: filelock in c:\\program files\\python311\\lib\\site-packages (from huggingface-hub>=0.30.2->langchain-huggingface) (3.18.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in c:\\program files\\python311\\lib\\site-packages (from huggingface-hub>=0.30.2->langchain-huggingface) (2025.3.2)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in c:\\program files\\python311\\lib\\site-packages (from huggingface-hub>=0.30.2->langchain-huggingface) (4.67.1)\n",
            "Requirement already satisfied: colorama in c:\\program files\\python311\\lib\\site-packages (from tqdm>=4.42.1->huggingface-hub>=0.30.2->langchain-huggingface) (0.4.6)\n",
            "Requirement already satisfied: sniffio>=1.1 in c:\\program files\\python311\\lib\\site-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.4,>=0.3.45->langchain-core<1.0.0,>=0.3.65->langchain-huggingface) (1.3.1)\n",
            "Requirement already satisfied: langchain-google-genai in c:\\program files\\python311\\lib\\site-packages (2.1.5)\n",
            "Requirement already satisfied: filetype<2.0.0,>=1.2.0 in c:\\program files\\python311\\lib\\site-packages (from langchain-google-genai) (1.2.0)\n",
            "Requirement already satisfied: google-ai-generativelanguage<0.7.0,>=0.6.18 in c:\\program files\\python311\\lib\\site-packages (from langchain-google-genai) (0.6.18)\n",
            "Requirement already satisfied: langchain-core<0.4.0,>=0.3.62 in c:\\program files\\python311\\lib\\site-packages (from langchain-google-genai) (0.3.65)\n",
            "Requirement already satisfied: pydantic<3,>=2 in c:\\program files\\python311\\lib\\site-packages (from langchain-google-genai) (2.11.5)\n",
            "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1 in c:\\program files\\python311\\lib\\site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai) (2.25.1)\n",
            "Requirement already satisfied: google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1 in c:\\program files\\python311\\lib\\site-packages (from google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai) (2.17.3)\n",
            "Requirement already satisfied: proto-plus<2.0.0,>=1.22.3 in c:\\program files\\python311\\lib\\site-packages (from google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai) (1.26.1)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0,>=3.20.2 in c:\\program files\\python311\\lib\\site-packages (from google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai) (6.31.1)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in c:\\program files\\python311\\lib\\site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai) (1.70.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.18.0 in c:\\program files\\python311\\lib\\site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai) (2.32.4)\n",
            "Requirement already satisfied: grpcio<2.0.0,>=1.33.2 in c:\\program files\\python311\\lib\\site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai) (1.73.0)\n",
            "Requirement already satisfied: grpcio-status<2.0.0,>=1.33.2 in c:\\program files\\python311\\lib\\site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai) (1.73.0)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in c:\\program files\\python311\\lib\\site-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai) (5.3.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\program files\\python311\\lib\\site-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai) (0.2.8)\n",
            "Requirement already satisfied: six>=1.9.0 in c:\\program files\\python311\\lib\\site-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai) (1.16.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\program files\\python311\\lib\\site-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai) (4.9)\n",
            "Requirement already satisfied: langsmith<0.4,>=0.3.45 in c:\\program files\\python311\\lib\\site-packages (from langchain-core<0.4.0,>=0.3.62->langchain-google-genai) (0.3.45)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in c:\\program files\\python311\\lib\\site-packages (from langchain-core<0.4.0,>=0.3.62->langchain-google-genai) (9.1.2)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in c:\\program files\\python311\\lib\\site-packages (from langchain-core<0.4.0,>=0.3.62->langchain-google-genai) (1.33)\n",
            "Requirement already satisfied: PyYAML>=5.3 in c:\\program files\\python311\\lib\\site-packages (from langchain-core<0.4.0,>=0.3.62->langchain-google-genai) (6.0.2)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in c:\\program files\\python311\\lib\\site-packages (from langchain-core<0.4.0,>=0.3.62->langchain-google-genai) (24.2)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in c:\\program files\\python311\\lib\\site-packages (from langchain-core<0.4.0,>=0.3.62->langchain-google-genai) (4.13.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in c:\\program files\\python311\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.62->langchain-google-genai) (3.0.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\program files\\python311\\lib\\site-packages (from langsmith<0.4,>=0.3.45->langchain-core<0.4.0,>=0.3.62->langchain-google-genai) (0.28.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in c:\\program files\\python311\\lib\\site-packages (from langsmith<0.4,>=0.3.45->langchain-core<0.4.0,>=0.3.62->langchain-google-genai) (3.10.18)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in c:\\program files\\python311\\lib\\site-packages (from langsmith<0.4,>=0.3.45->langchain-core<0.4.0,>=0.3.62->langchain-google-genai) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in c:\\program files\\python311\\lib\\site-packages (from langsmith<0.4,>=0.3.45->langchain-core<0.4.0,>=0.3.62->langchain-google-genai) (0.23.0)\n",
            "Requirement already satisfied: anyio in c:\\program files\\python311\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.3.45->langchain-core<0.4.0,>=0.3.62->langchain-google-genai) (4.9.0)\n",
            "Requirement already satisfied: certifi in c:\\program files\\python311\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.3.45->langchain-core<0.4.0,>=0.3.62->langchain-google-genai) (2022.12.7)\n",
            "Requirement already satisfied: httpcore==1.* in c:\\program files\\python311\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.3.45->langchain-core<0.4.0,>=0.3.62->langchain-google-genai) (1.0.9)\n",
            "Requirement already satisfied: idna in c:\\program files\\python311\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.3.45->langchain-core<0.4.0,>=0.3.62->langchain-google-genai) (3.4)\n",
            "Requirement already satisfied: h11>=0.16 in c:\\program files\\python311\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.3.45->langchain-core<0.4.0,>=0.3.62->langchain-google-genai) (0.16.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in c:\\program files\\python311\\lib\\site-packages (from pydantic<3,>=2->langchain-google-genai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in c:\\program files\\python311\\lib\\site-packages (from pydantic<3,>=2->langchain-google-genai) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in c:\\program files\\python311\\lib\\site-packages (from pydantic<3,>=2->langchain-google-genai) (0.4.1)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\program files\\python311\\lib\\site-packages (from requests<3.0.0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai) (3.1.0)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\program files\\python311\\lib\\site-packages (from requests<3.0.0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai) (1.26.15)\n",
            "Requirement already satisfied: pyasn1>=0.1.3 in c:\\program files\\python311\\lib\\site-packages (from rsa<5,>=3.1.4->google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai) (0.4.8)\n",
            "Requirement already satisfied: sniffio>=1.1 in c:\\program files\\python311\\lib\\site-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.4,>=0.3.45->langchain-core<0.4.0,>=0.3.62->langchain-google-genai) (1.3.1)\n",
            "Requirement already satisfied: langchain_openai in c:\\program files\\python311\\lib\\site-packages (0.3.24)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.65 in c:\\program files\\python311\\lib\\site-packages (from langchain_openai) (0.3.65)\n",
            "Requirement already satisfied: openai<2.0.0,>=1.86.0 in c:\\program files\\python311\\lib\\site-packages (from langchain_openai) (1.88.0)\n",
            "Requirement already satisfied: tiktoken<1,>=0.7 in c:\\program files\\python311\\lib\\site-packages (from langchain_openai) (0.9.0)\n",
            "Requirement already satisfied: langsmith<0.4,>=0.3.45 in c:\\program files\\python311\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.65->langchain_openai) (0.3.45)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in c:\\program files\\python311\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.65->langchain_openai) (9.1.2)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in c:\\program files\\python311\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.65->langchain_openai) (1.33)\n",
            "Requirement already satisfied: PyYAML>=5.3 in c:\\program files\\python311\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.65->langchain_openai) (6.0.2)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in c:\\program files\\python311\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.65->langchain_openai) (24.2)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in c:\\program files\\python311\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.65->langchain_openai) (4.13.0)\n",
            "Requirement already satisfied: pydantic>=2.7.4 in c:\\program files\\python311\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.65->langchain_openai) (2.11.5)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in c:\\program files\\python311\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.65->langchain_openai) (3.0.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\program files\\python311\\lib\\site-packages (from langsmith<0.4,>=0.3.45->langchain-core<1.0.0,>=0.3.65->langchain_openai) (0.28.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in c:\\program files\\python311\\lib\\site-packages (from langsmith<0.4,>=0.3.45->langchain-core<1.0.0,>=0.3.65->langchain_openai) (3.10.18)\n",
            "Requirement already satisfied: requests<3,>=2 in c:\\program files\\python311\\lib\\site-packages (from langsmith<0.4,>=0.3.45->langchain-core<1.0.0,>=0.3.65->langchain_openai) (2.32.4)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in c:\\program files\\python311\\lib\\site-packages (from langsmith<0.4,>=0.3.45->langchain-core<1.0.0,>=0.3.65->langchain_openai) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in c:\\program files\\python311\\lib\\site-packages (from langsmith<0.4,>=0.3.45->langchain-core<1.0.0,>=0.3.65->langchain_openai) (0.23.0)\n",
            "Requirement already satisfied: anyio in c:\\program files\\python311\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.3.45->langchain-core<1.0.0,>=0.3.65->langchain_openai) (4.9.0)\n",
            "Requirement already satisfied: certifi in c:\\program files\\python311\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.3.45->langchain-core<1.0.0,>=0.3.65->langchain_openai) (2022.12.7)\n",
            "Requirement already satisfied: httpcore==1.* in c:\\program files\\python311\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.3.45->langchain-core<1.0.0,>=0.3.65->langchain_openai) (1.0.9)\n",
            "Requirement already satisfied: idna in c:\\program files\\python311\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.3.45->langchain-core<1.0.0,>=0.3.65->langchain_openai) (3.4)\n",
            "Requirement already satisfied: h11>=0.16 in c:\\program files\\python311\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.3.45->langchain-core<1.0.0,>=0.3.65->langchain_openai) (0.16.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in c:\\program files\\python311\\lib\\site-packages (from openai<2.0.0,>=1.86.0->langchain_openai) (1.9.0)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in c:\\program files\\python311\\lib\\site-packages (from openai<2.0.0,>=1.86.0->langchain_openai) (0.10.0)\n",
            "Requirement already satisfied: sniffio in c:\\program files\\python311\\lib\\site-packages (from openai<2.0.0,>=1.86.0->langchain_openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in c:\\program files\\python311\\lib\\site-packages (from openai<2.0.0,>=1.86.0->langchain_openai) (4.67.1)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in c:\\program files\\python311\\lib\\site-packages (from pydantic>=2.7.4->langchain-core<1.0.0,>=0.3.65->langchain_openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in c:\\program files\\python311\\lib\\site-packages (from pydantic>=2.7.4->langchain-core<1.0.0,>=0.3.65->langchain_openai) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in c:\\program files\\python311\\lib\\site-packages (from pydantic>=2.7.4->langchain-core<1.0.0,>=0.3.65->langchain_openai) (0.4.1)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\program files\\python311\\lib\\site-packages (from requests<3,>=2->langsmith<0.4,>=0.3.45->langchain-core<1.0.0,>=0.3.65->langchain_openai) (3.1.0)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\program files\\python311\\lib\\site-packages (from requests<3,>=2->langsmith<0.4,>=0.3.45->langchain-core<1.0.0,>=0.3.65->langchain_openai) (1.26.15)\n",
            "Requirement already satisfied: regex>=2022.1.18 in c:\\program files\\python311\\lib\\site-packages (from tiktoken<1,>=0.7->langchain_openai) (2024.11.6)\n",
            "Requirement already satisfied: colorama in c:\\program files\\python311\\lib\\site-packages (from tqdm>4->openai<2.0.0,>=1.86.0->langchain_openai) (0.4.6)\n",
            "Requirement already satisfied: openai in c:\\program files\\python311\\lib\\site-packages (1.88.0)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in c:\\program files\\python311\\lib\\site-packages (from openai) (4.9.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in c:\\program files\\python311\\lib\\site-packages (from openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\program files\\python311\\lib\\site-packages (from openai) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in c:\\program files\\python311\\lib\\site-packages (from openai) (0.10.0)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in c:\\program files\\python311\\lib\\site-packages (from openai) (2.11.5)\n",
            "Requirement already satisfied: sniffio in c:\\program files\\python311\\lib\\site-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in c:\\program files\\python311\\lib\\site-packages (from openai) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in c:\\program files\\python311\\lib\\site-packages (from openai) (4.13.0)\n",
            "Requirement already satisfied: idna>=2.8 in c:\\program files\\python311\\lib\\site-packages (from anyio<5,>=3.5.0->openai) (3.4)\n",
            "Requirement already satisfied: certifi in c:\\program files\\python311\\lib\\site-packages (from httpx<1,>=0.23.0->openai) (2022.12.7)\n",
            "Requirement already satisfied: httpcore==1.* in c:\\program files\\python311\\lib\\site-packages (from httpx<1,>=0.23.0->openai) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in c:\\program files\\python311\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.16.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in c:\\program files\\python311\\lib\\site-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in c:\\program files\\python311\\lib\\site-packages (from pydantic<3,>=1.9.0->openai) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in c:\\program files\\python311\\lib\\site-packages (from pydantic<3,>=1.9.0->openai) (0.4.1)\n",
            "Requirement already satisfied: colorama in c:\\program files\\python311\\lib\\site-packages (from tqdm>4->openai) (0.4.6)\n",
            "Requirement already satisfied: pinecone in c:\\program files\\python311\\lib\\site-packages (7.1.0)\n",
            "Collecting pinecone\n",
            "  Downloading pinecone-7.2.0-py3-none-any.whl.metadata (9.5 kB)\n",
            "Requirement already satisfied: certifi>=2019.11.17 in c:\\program files\\python311\\lib\\site-packages (from pinecone) (2022.12.7)\n",
            "Requirement already satisfied: pinecone-plugin-assistant<2.0.0,>=1.6.0 in c:\\program files\\python311\\lib\\site-packages (from pinecone) (1.7.0)\n",
            "Requirement already satisfied: pinecone-plugin-interface<0.0.8,>=0.0.7 in c:\\program files\\python311\\lib\\site-packages (from pinecone) (0.0.7)\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in c:\\program files\\python311\\lib\\site-packages (from pinecone) (2.8.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4 in c:\\program files\\python311\\lib\\site-packages (from pinecone) (4.13.0)\n",
            "Requirement already satisfied: urllib3>=1.26.0 in c:\\program files\\python311\\lib\\site-packages (from pinecone) (1.26.15)\n",
            "Requirement already satisfied: packaging<25.0,>=24.2 in c:\\program files\\python311\\lib\\site-packages (from pinecone-plugin-assistant<2.0.0,>=1.6.0->pinecone) (24.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.32.3 in c:\\program files\\python311\\lib\\site-packages (from pinecone-plugin-assistant<2.0.0,>=1.6.0->pinecone) (2.32.4)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\program files\\python311\\lib\\site-packages (from requests<3.0.0,>=2.32.3->pinecone-plugin-assistant<2.0.0,>=1.6.0->pinecone) (3.1.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\program files\\python311\\lib\\site-packages (from requests<3.0.0,>=2.32.3->pinecone-plugin-assistant<2.0.0,>=1.6.0->pinecone) (3.4)\n",
            "Requirement already satisfied: six>=1.5 in c:\\program files\\python311\\lib\\site-packages (from python-dateutil>=2.5.3->pinecone) (1.16.0)\n",
            "Downloading pinecone-7.2.0-py3-none-any.whl (524 kB)\n",
            "   ---------------------------------------- 0.0/524.3 kB ? eta -:--:--\n",
            "   --------------------------------------- 524.3/524.3 kB 16.4 MB/s eta 0:00:00\n",
            "Installing collected packages: pinecone\n",
            "  Attempting uninstall: pinecone\n",
            "    Found existing installation: pinecone 7.1.0\n",
            "    Uninstalling pinecone-7.1.0:\n",
            "      Successfully uninstalled pinecone-7.1.0\n",
            "Successfully installed pinecone-7.2.0\n",
            "Requirement already satisfied: flashrank in c:\\program files\\python311\\lib\\site-packages (0.2.10)\n",
            "Requirement already satisfied: tokenizers in c:\\program files\\python311\\lib\\site-packages (from flashrank) (0.21.1)\n",
            "Requirement already satisfied: onnxruntime in c:\\program files\\python311\\lib\\site-packages (from flashrank) (1.22.0)\n",
            "Requirement already satisfied: numpy in c:\\program files\\python311\\lib\\site-packages (from flashrank) (1.26.4)\n",
            "Requirement already satisfied: requests in c:\\program files\\python311\\lib\\site-packages (from flashrank) (2.32.4)\n",
            "Requirement already satisfied: tqdm in c:\\program files\\python311\\lib\\site-packages (from flashrank) (4.67.1)\n",
            "Requirement already satisfied: coloredlogs in c:\\program files\\python311\\lib\\site-packages (from onnxruntime->flashrank) (15.0.1)\n",
            "Requirement already satisfied: flatbuffers in c:\\program files\\python311\\lib\\site-packages (from onnxruntime->flashrank) (23.3.3)\n",
            "Requirement already satisfied: packaging in c:\\program files\\python311\\lib\\site-packages (from onnxruntime->flashrank) (24.2)\n",
            "Requirement already satisfied: protobuf in c:\\program files\\python311\\lib\\site-packages (from onnxruntime->flashrank) (6.31.1)\n",
            "Requirement already satisfied: sympy in c:\\program files\\python311\\lib\\site-packages (from onnxruntime->flashrank) (1.14.0)\n",
            "Requirement already satisfied: humanfriendly>=9.1 in c:\\program files\\python311\\lib\\site-packages (from coloredlogs->onnxruntime->flashrank) (10.0)\n",
            "Requirement already satisfied: pyreadline3 in c:\\program files\\python311\\lib\\site-packages (from humanfriendly>=9.1->coloredlogs->onnxruntime->flashrank) (3.5.4)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\program files\\python311\\lib\\site-packages (from requests->flashrank) (3.1.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\program files\\python311\\lib\\site-packages (from requests->flashrank) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\program files\\python311\\lib\\site-packages (from requests->flashrank) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\program files\\python311\\lib\\site-packages (from requests->flashrank) (2022.12.7)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\program files\\python311\\lib\\site-packages (from sympy->onnxruntime->flashrank) (1.3.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in c:\\program files\\python311\\lib\\site-packages (from tokenizers->flashrank) (0.33.0)\n",
            "Requirement already satisfied: filelock in c:\\program files\\python311\\lib\\site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers->flashrank) (3.18.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in c:\\program files\\python311\\lib\\site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers->flashrank) (2025.3.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in c:\\program files\\python311\\lib\\site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers->flashrank) (6.0.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\program files\\python311\\lib\\site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers->flashrank) (4.13.0)\n",
            "Requirement already satisfied: colorama in c:\\program files\\python311\\lib\\site-packages (from tqdm->flashrank) (0.4.6)\n"
          ]
        }
      ],
      "source": [
        "# @title Default title text\n",
        "!pip install langgraph langsmith langchain langchain_community\n",
        "!pip install faiss-cpu\n",
        "!pip install sentence-transformers\n",
        "!pip install -U langchain-huggingface\n",
        "!pip install -U langchain-google-genai\n",
        "!pip install -U langchain_openai\n",
        "!pip install -U openai\n",
        "!pip install -U pinecone\n",
        "!pip install flashrank"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V9JnlZTf-btn"
      },
      "source": [
        "## PINECONE DB RETRIEVER"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "yMo-66Ei-bto"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from dotenv import load_dotenv\n",
        "from pinecone import Pinecone\n",
        "from typing import List, Any\n",
        "from pydantic import BaseModel, Field\n",
        "\n",
        "from langchain.schema import BaseRetriever, Document\n",
        "from langchain.load import dumps, loads\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "\n",
        "\n",
        "class PineconeDBRetriever(BaseRetriever, BaseModel):\n",
        "    \"\"\"\n",
        "    A custom LangChain retriever for Pinecone.\n",
        "    \"\"\"\n",
        "    index_name: str\n",
        "    pinecone_api_key: str\n",
        "    namespace: str\n",
        "    top_k: int = 5\n",
        "    index: Any = Field(None, exclude=True)\n",
        "\n",
        "    def __init__(self, **data):\n",
        "        \"\"\"\n",
        "        Initializes the Pinecone client and index.\n",
        "        \"\"\"\n",
        "        super().__init__(**data)\n",
        "        pc = Pinecone(api_key=self.pinecone_api_key)\n",
        "        self.index = pc.Index(self.index_name)\n",
        "\n",
        "    def _get_relevant_documents(self, query: str) -> List[Document]:\n",
        "        \"\"\"\n",
        "        The core method to retrieve documents. LangChain's retriever system\n",
        "        calls this method.\n",
        "\n",
        "        Args:\n",
        "            query (str): The user's question.\n",
        "\n",
        "        Returns:\n",
        "            List[Document]: A list of relevant documents from Pinecone.\n",
        "        \"\"\"\n",
        "        # Pinecone's hosted embedding model will automatically embed the query text.\n",
        "        results = self.index.search(\n",
        "            namespace=self.namespace,\n",
        "            query={\n",
        "                \"inputs\": {\"text\": query},\n",
        "                \"top_k\": self.top_k\n",
        "            }\n",
        "        )\n",
        "\n",
        "        # Convert Pinecone's search results into LangChain Document objects.\n",
        "        # TODO: Add additional fields as necessary.\n",
        "        documents = []\n",
        "        if results and 'result' in results and 'hits' in results['result']:\n",
        "            for match in results['result']['hits']:\n",
        "                # The actual text content is in the 'fields' dictionary\n",
        "                page_content = match.get('fields', {}).get('text', '')\n",
        "                # metadata = {\"id\": match.get(\"_id\"), \"score\": match.get(\"_score\")}\n",
        "                metadata = {\"id\": match.get(\"_id\")} # Removing score to allow easy serialization and help de-duplication\n",
        "\n",
        "                doc = Document(\n",
        "                    page_content=page_content,\n",
        "                    metadata=metadata\n",
        "                )\n",
        "                documents.append(doc)\n",
        "\n",
        "        return documents\n",
        "\n",
        "    async def _aget_relevant_documents(self, query: str) -> List[Document]:\n",
        "        \"\"\"\n",
        "        Asynchronous version of the document retrieval method.\n",
        "        \"\"\"\n",
        "        # For simplicity, we'll just call the synchronous version.\n",
        "        # For a production environment, you might want to use an async Pinecone client.\n",
        "        return self._get_relevant_documents(query)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D33r0fv9-btp"
      },
      "source": [
        "## RAG ORCHESTRATOR"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "DpJuH5l9-btp"
      },
      "outputs": [],
      "source": [
        "from operator import itemgetter\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.runnables import RunnableLambda, RunnablePassthrough\n",
        "from langchain_core.runnables import RunnableConfig, RunnableParallel\n",
        "from langchain_openai import ChatOpenAI\n",
        "from pprint import pprint\n",
        "from typing import Any, List\n",
        "from langchain.load import dumps, loads\n",
        "from sentence_transformers.cross_encoder import CrossEncoder\n",
        "from langchain.retrievers import ContextualCompressionRetriever\n",
        "from langchain.retrievers.document_compressors import LLMChainExtractor\n",
        "\n",
        "class RAGOrchestrator:\n",
        "    \"\"\"\n",
        "    Orchestrates the RAG pipeline based on a given configuration.\n",
        "    \"\"\"\n",
        "    def __init__(self, config: dict):\n",
        "        \"\"\"\n",
        "        Initializes the orchestrator with a configuration dictionary.\n",
        "\n",
        "        Args:\n",
        "            config (dict): A dictionary containing settings for the RAG pipeline,\n",
        "                           such as model name, index name, and retrieval strategy.\n",
        "        \"\"\"\n",
        "        self.config = config\n",
        "        self.debug = config.get(\"debug\", False)\n",
        "        self.llm = ChatOpenAI(\n",
        "            model=config.get(\"llm_model\", \"gpt-4o-mini\"),\n",
        "            api_key=OPENAI_API_KEY\n",
        "        )\n",
        "        if config.get(\"retrieval_strategy\") != \"llm_only\":\n",
        "            self.retriever = PineconeDBRetriever(\n",
        "                index_name=config.get(\"index_name\"),\n",
        "                pinecone_api_key=PINECONE_API_KEY,\n",
        "                namespace=config.get(\"namespace\"),\n",
        "                top_k=config.get(\"top_k\", 5)\n",
        "            )\n",
        "\n",
        "    # --- Debugging Helper ---\n",
        "    def _print_debug(self, header: str, data: Any):\n",
        "        if self.debug:\n",
        "            print(\"\\n\" + \"=\"*20)\n",
        "            print(f\"DEBUG: {header}\")\n",
        "            print(\"=\"*20)\n",
        "            pprint(data)\n",
        "        return data # Pass data through unchanged\n",
        "\n",
        "    def _tap_and_log(self, x: dict) -> dict:\n",
        "        \"\"\"\n",
        "        A helper method to print debug info and pass the input dictionary through unchanged.\n",
        "        \"\"\"\n",
        "        self._print_debug(\"Final Context for LLM\", x.get(\"context_str\", \"Context not available\"))\n",
        "        return x\n",
        "\n",
        "    def _get_unique_union(self, documents: list[list]) -> List[Document]:\n",
        "        \"\"\"\n",
        "        Takes a list of document lists, merges them, and removes duplicates.\n",
        "        \"\"\"\n",
        "        # Serialize each document to a string to make them hashable\n",
        "        flattened_docs = [dumps(doc) for sublist in documents for doc in sublist]\n",
        "        # Use a set to get unique serialized documents\n",
        "        unique_docs = list(set(flattened_docs))\n",
        "        # Deserialize unique documents back into Document objects\n",
        "        return [loads(doc) for doc in unique_docs]\n",
        "\n",
        "    # --- Retrieval Strategy Helpers ---\n",
        "\n",
        "    def _get_multi_query_chain(self):\n",
        "        # Builds a chain that generates multiple queries and retrieves documents for each.\n",
        "        # 1. Prompt for generating multiple queries\n",
        "        template = \"\"\"You are an AI language model assistant. Your task is to generate five\n",
        "        different versions of the given user question to retrieve relevant documents from a vector\n",
        "        database. By generating multiple perspectives on the user question, your goal is to help\n",
        "        the user overcome some of the limitations of the cosine-based similarity search.\n",
        "        Provide these alternative questions separated by newlines. Original question: {question}\"\"\"\n",
        "        prompt_perspectives = ChatPromptTemplate.from_template(template)\n",
        "\n",
        "        # 3. The chain for generating and retrieving\n",
        "        generate_queries = (\n",
        "            prompt_perspectives\n",
        "            | self.llm\n",
        "            | StrOutputParser()\n",
        "            | (lambda x: x.split(\"\\n\"))\n",
        "            | RunnableLambda(lambda x: [q for q in x if q.strip()])\n",
        "            | RunnableLambda(lambda x: self._print_debug(\"Generated Queries\", x))\n",
        "        )\n",
        "\n",
        "        retrieval_chain = generate_queries | self.retriever.map() | self._get_unique_union | RunnableLambda(lambda docs: self._print_debug(\"Retrieved Documents\", docs))\n",
        "        return retrieval_chain\n",
        "\n",
        "    def _get_rag_fusion_chain(self):\n",
        "        \"\"\"Builds a chain for RAG Fusion with reciprocal rank fusion.\"\"\"\n",
        "        # 1. The multi-query generation is the same as above\n",
        "        template = \"\"\"You are an AI language model assistant. Your task is to generate five\n",
        "        different versions of the given user question to retrieve relevant documents from a vector\n",
        "        database. By generating multiple perspectives on the user question, your goal is to help\n",
        "        the user overcome some of the limitations of the cosine-based similarity search.\n",
        "        Provide these alternative questions separated by newlines. Original question: {question}\"\"\"\n",
        "        prompt_perspectives = ChatPromptTemplate.from_template(template)\n",
        "\n",
        "        generate_queries = (\n",
        "            prompt_perspectives\n",
        "            | self.llm\n",
        "            | StrOutputParser()\n",
        "            | (lambda x: x.split(\"\\n\"))\n",
        "            | RunnableLambda(lambda x: [q for q in x if q.strip()])\n",
        "            | RunnableLambda(lambda x: self._print_debug(\"Generated Queries\", x))\n",
        "        )\n",
        "\n",
        "        # 2. Reranking with Reciprocal Rank Fusion\n",
        "        def reciprocal_rank_fusion(results: list[list], k=60):\n",
        "            fused_scores = {}\n",
        "            for docs in results:\n",
        "                for rank, doc in enumerate(docs):\n",
        "                    doc_str = dumps(doc)\n",
        "                    if doc_str not in fused_scores:\n",
        "                        fused_scores[doc_str] = 0\n",
        "                    fused_scores[doc_str] += 1 / (rank + k)\n",
        "\n",
        "            # .item() converts [doc_str: score] pairs to a list of tuples [doc_str, score]\n",
        "            # Sort by score in descending order (reverse=True)\n",
        "            reranked_results = [\n",
        "                (loads(doc), score)\n",
        "                for doc, score in sorted(fused_scores.items(), key=lambda x: x[1], reverse=True)\n",
        "            ]\n",
        "            self._print_debug(\"Reranked Documents (RAG Fusion)\", reranked_results)\n",
        "            # Return only the documents, not the scores\n",
        "            return [doc for doc, score in reranked_results]\n",
        "\n",
        "        # 3. The RAG Fusion chain\n",
        "        retrieval_chain = generate_queries | self.retriever.map() | reciprocal_rank_fusion\n",
        "        return retrieval_chain\n",
        "\n",
        "    def _get_decomposition_chain(self):\n",
        "        # Builds a chain that decomposes a question into sub-questions.\n",
        "        # 1. Prompt for generating sub-questions\n",
        "        decomposition_template = \"\"\"You are a helpful assistant that generates multiple sub-questions\n",
        "        related to an input question. The goal is to break down the input into a set of sub-problems\n",
        "        that can be answered in isolation. Generate multiple search queries related to: {question}\n",
        "        Output (separated by newlines):\"\"\"\n",
        "        prompt_decomposition = ChatPromptTemplate.from_template(decomposition_template)\n",
        "\n",
        "        # 2. Chain to generate and clean up sub-questions\n",
        "        generate_queries_decomposition = (\n",
        "            prompt_decomposition\n",
        "            | self.llm\n",
        "            | StrOutputParser()\n",
        "            | (lambda x: x.split(\"\\n\"))\n",
        "            | RunnableLambda(lambda x: [q for q in x if q.strip()])\n",
        "            | RunnableLambda(lambda x: self._print_debug(\"Decomposed Sub-questions\", x))\n",
        "        )\n",
        "\n",
        "        # 3. The full retrieval chain using the decomposed questions\n",
        "        retrieval_chain = (\n",
        "            generate_queries_decomposition\n",
        "            | self.retriever.map()\n",
        "            | self._get_unique_union\n",
        "            | RunnableLambda(lambda docs: self._print_debug(\"Retrieved Documents (Decomposition)\", docs)\n",
        "            ))\n",
        "        return retrieval_chain\n",
        "\n",
        "    def _get_step_back_chain(self):\n",
        "        # Builds a chain that generates a general, \"stepped-back\" question and retrieves documents for it.\n",
        "        # 1. Prompt to generate a more general, \"stepped-back\" question\n",
        "        step_back_template = \"\"\"You are an expert at world knowledge. Your task is to step back and\n",
        "        paraphrase a question to a more generic step-back question, which is easier to answer.\n",
        "\n",
        "        Here are a few examples:\n",
        "        Original Question: What is the C29x CPU architecture in the F29H85x microcontroller?\n",
        "        Step-Back Question: What are the technical specifications of the C29x CPU architecture?\n",
        "\n",
        "        Original Question: Which TI device was recommended for automotive radar in the 2023 safety seminar?\n",
        "        Step-Back Question: What are some common TI devices used for automotive radar applications?\n",
        "\n",
        "        Original Question: {question}\n",
        "        Step-Back Question:\"\"\"\n",
        "        prompt_step_back = ChatPromptTemplate.from_template(step_back_template)\n",
        "\n",
        "        # 2. Chain to generate the new question\n",
        "        generate_step_back_query = (\n",
        "            prompt_step_back\n",
        "            | self.llm\n",
        "            | StrOutputParser()\n",
        "            #| (lambda x: x.split(\"\\n\"))\n",
        "            #| RunnableLambda(lambda x: [q for q in x if q.strip()])\n",
        "            | RunnableLambda(lambda x: self._print_debug(\"Generated Step-Back Question\", x))\n",
        "        )\n",
        "\n",
        "        # 3. The full retrieval chain using the new question\n",
        "        # This takes the original question, generates a new one, and retrieves docs with it\n",
        "        retrieval_chain = generate_step_back_query | self.retriever | RunnableLambda(lambda docs: self._print_debug(\"Retrieved Documents (Step back)\", docs))\n",
        "        return retrieval_chain\n",
        "\n",
        "    def _get_hyde_chain(self):\n",
        "        # Builds a chain that generates a hypothetical document and uses it for retrieval.\n",
        "\n",
        "        # 1. Prompt to generate a hypothetical document (a plausible answer)\n",
        "        hyde_template = \"\"\"Please write a passage to answer the user's question.\n",
        "        This passage should be detailed and informative, as if it came from a technical document.\n",
        "        The purpose is to create a rich text for a vector search.\n",
        "\n",
        "        Question: {question}\n",
        "        Passage:\"\"\"\n",
        "        prompt_hyde = ChatPromptTemplate.from_template(hyde_template)\n",
        "\n",
        "        # 2. Chain to generate the hypothetical document\n",
        "        generate_hyde_document = (\n",
        "            prompt_hyde\n",
        "            | self.llm\n",
        "            | StrOutputParser()\n",
        "            | RunnableLambda(lambda x: self._print_debug(\"Generated Hypothetical Document\", x))\n",
        "        )\n",
        "\n",
        "        # 3. The full retrieval chain: generate a hypothetical doc, then retrieve with it\n",
        "        retrieval_chain = generate_hyde_document | self.retriever | RunnableLambda(lambda docs: self._print_debug(\"Retrieved Documents (HyDE)\", docs))\n",
        "        return retrieval_chain\n",
        "\n",
        "    # --- Post Retrieval Processing Helpers ---\n",
        "    \n",
        "    def _get_st_reranking_chain(self):\n",
        "        \"\"\"\n",
        "        Creates a Runnable that performs semantic re-ranking using a\n",
        "        Cross-Encoder model from the sentence-transformers library.\n",
        "        \"\"\"\n",
        "        # Initialize a cross-encoder model. This model is lightweight and effective.\n",
        "        # It will be downloaded from the Hugging Face Hub on first use.\n",
        "        model = CrossEncoder('cross-encoder/ms-marco-MiniLM-L-6-v2')\n",
        "\n",
        "        def rerank_docs(inputs: dict):\n",
        "            documents = inputs.get(\"documents\", [])\n",
        "            query = inputs.get(\"query\", \"\")\n",
        "            if not documents or not query:\n",
        "                return []\n",
        "\n",
        "            self._print_debug(f\"Documents going INTO Re-ranker ({len(documents)} docs)\", documents)\n",
        "\n",
        "            # 1. Create pairs of [query, passage] for the cross-encoder\n",
        "            sentence_pairs = [(query, doc.page_content) for doc in documents]\n",
        "\n",
        "            # 2. Predict the relevance scores. The output is a list of scores.\n",
        "            scores = model.predict(sentence_pairs)\n",
        "\n",
        "            # 3. Combine the original documents with their new scores\n",
        "            scored_docs = list(zip(scores, documents))\n",
        "\n",
        "            # 4. Sort the documents by score in descending order\n",
        "            scored_docs.sort(key=lambda x: x[0], reverse=True)\n",
        "\n",
        "            # 5. Extract the documents and limit by top_n\n",
        "            reranked_docs = [doc for score, doc in scored_docs]\n",
        "            configured_top_n = self.config.get(\"reranker_top_n\", 5)\n",
        "            effective_top_n = min(configured_top_n, len(reranked_docs))\n",
        "            final_docs_to_return = reranked_docs[:effective_top_n]\n",
        "            \n",
        "            self._print_debug(f\"Documents COMING OUT of Re-ranker ({len(final_docs_to_return)} docs)\", final_docs_to_return)\n",
        "            return final_docs_to_return\n",
        "\n",
        "        return RunnableLambda(rerank_docs)\n",
        "    \n",
        "    def _get_contextual_compression_retriever(self, base_retriever):\n",
        "        \"\"\"\n",
        "        Takes a base retriever and wraps it with a compressor.\n",
        "        \"\"\"\n",
        "        # 1. Initialize the compressor. This component uses an LLM to read each retrieved document and extract only the sentences relevant to the query.\n",
        "        compressor = LLMChainExtractor.from_llm(self.llm)\n",
        "\n",
        "        # 2. Create the compression retriever. This is a wrapper that first runs the base_retriever, then passes the results to the compressor.\n",
        "        compression_retriever = ContextualCompressionRetriever(\n",
        "            base_compressor=compressor,\n",
        "            base_retriever=base_retriever\n",
        "        )\n",
        "        \n",
        "        self._print_debug(\"Contextual Compression Retriever\", \"Initialized and ready.\")\n",
        "        return compression_retriever    \n",
        "\n",
        "    # --- Propmt Stregy Helpers ---\n",
        "    def _get_final_prompt(self):\n",
        "        \"\"\"\n",
        "        Selects and returns the final prompt template based on the config.\n",
        "        \"\"\"\n",
        "        prompt_strategy = self.config.get(\"prompt_strategy\", \"strict_context\")\n",
        "        \n",
        "        if prompt_strategy == \"permissive_context\":\n",
        "            # This prompt allows the LLM to use its own knowledge\n",
        "            template = \"\"\"You are a helpful expert assistant for Texas Instruments products.\n",
        "            Answer the user's question based on the context provided.\n",
        "            If the context is not sufficient to answer the question, use your own knowledge to provide a comprehensive answer,\n",
        "            but you must state that the information comes from your general knowledge.\n",
        "            \n",
        "            Context: {context}\n",
        "            Question: {question}\n",
        "            \"\"\"\n",
        "        else: # Default to \"strict_context\"\n",
        "            # This prompt forces the LLM to only use the provided documents\n",
        "            template = \"\"\"Answer the following question based ONLY on the provided context.\n",
        "            If the answer is not available in the context, you must say \"Based on the provided context, I cannot answer this question.\"\n",
        "            \n",
        "            Context: {context}\n",
        "            Question: {question}\n",
        "            \"\"\"\n",
        "            \n",
        "        self._print_debug(f\"Using Prompt Strategy: {prompt_strategy}\", template)\n",
        "        return ChatPromptTemplate.from_template(template)\n",
        "    \n",
        "    def invoke(self, question: str) -> dict:\n",
        "        \"\"\"\n",
        "        Builds and invokes the RAG chain based on the configuration.\n",
        "\n",
        "        Args:\n",
        "            question (str): The user's question.\n",
        "\n",
        "        Returns:\n",
        "            dict: A dictionary containing the question, retrieved context, and the final answer.\n",
        "        \"\"\"\n",
        "        strategy = self.config.get(\"retrieval_strategy\", \"simple\")\n",
        "        post_processing_strategy = self.config.get(\"post_retrieval_processing\", \"none\")\n",
        "        prompt_strategy = self.config.get(\"prompt_strategy\", \"strict_context\")\n",
        "\n",
        "        # LLM_only strategy does not use retrieval\n",
        "        if strategy == \"llm_only\":\n",
        "            self._print_debug(\"Strategy\", \"LLM Only (No RAG)\")\n",
        "            answer = self.llm.invoke(question).content\n",
        "            return {\n",
        "                \"question\": question,\n",
        "                \"answer\": answer,\n",
        "                \"strategy\": strategy,\n",
        "                \"context\": \"N/A\" # No context was used\n",
        "            }\n",
        "\n",
        "        # --- For RAG-based strategies ---\n",
        "\n",
        "        # Select the base retrieval chain (gets the initial list of documents)\n",
        "        if strategy == \"multi_query\":\n",
        "            base_retrieval_chain = self._get_multi_query_chain()\n",
        "        elif strategy == \"rag_fusion\":\n",
        "            base_retrieval_chain = self._get_rag_fusion_chain()\n",
        "        elif strategy == \"decomposition\":\n",
        "            base_retrieval_chain = self._get_decomposition_chain()\n",
        "        elif strategy == \"step_back\":\n",
        "            base_retrieval_chain = self._get_step_back_chain()\n",
        "        elif strategy == \"hyde\":\n",
        "            base_retrieval_chain = self._get_hyde_chain()\n",
        "        # -----------------------------\n",
        "        else: # Default to simple retrieval\n",
        "            base_retrieval_chain = self.retriever | RunnableLambda(\n",
        "                lambda docs: self._print_debug(\"Retrieved Documents (Simple)\", docs)\n",
        "            )\n",
        "\n",
        "        # Conditionally apply post-processing\n",
        "        # If no post-processing is specified, use the base retrieval chain as is\n",
        "        final_retrieval_chain = base_retrieval_chain\n",
        "        \n",
        "        if \"semantic_re_ranking\" in post_processing_strategy:\n",
        "            # The re-ranker needs both docs and query\n",
        "            reranker_chain = {\"documents\": final_retrieval_chain, \"query\": RunnablePassthrough()} | self._get_st_reranking_chain()\n",
        "            final_retrieval_chain = reranker_chain\n",
        "        \n",
        "        # Conditionally apply the compression layer\n",
        "        if \"contextual_compression\" in post_processing_strategy:\n",
        "            # The compression retriever wraps the base retriever\n",
        "            final_retrieval_chain = self._get_contextual_compression_retriever(final_retrieval_chain)\n",
        "\n",
        "\n",
        "        # Final chains for invoking the LLM\n",
        "        final_prompt = self._get_final_prompt()\n",
        "\n",
        "        def format_docs(docs: List[Document]) -> str:\n",
        "            return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
        "\n",
        "        context_formatter = (\n",
        "            RunnablePassthrough.assign(\n",
        "                context_str=itemgetter(\"context\") | RunnableLambda(format_docs)\n",
        "            )\n",
        "            #| RunnableLambda(self._tap_and_log)\n",
        "        )        \n",
        "        \n",
        "        rag_chain = (\n",
        "            {\"context\": final_retrieval_chain, \"question\": RunnablePassthrough()}\n",
        "            | context_formatter\n",
        "            | {\n",
        "                  \"answer\": (\n",
        "                      lambda x: {\"context\": x[\"context_str\"], \"question\": x[\"question\"]}\n",
        "                  ) | final_prompt | self.llm | StrOutputParser(),\n",
        "                  \"context\": itemgetter(\"context\"),\n",
        "              }\n",
        "        )\n",
        "\n",
        "        result = rag_chain.invoke(question)\n",
        "\n",
        "        return {\n",
        "            \"question\": question,\n",
        "            \"answer\": result['answer'],\n",
        "            \"strategy\": f\"{strategy} RAG + {post_processing_strategy} Post-Retrieval Processing + {prompt_strategy} Prompt Strategy\",\n",
        "            \"context\": result['context']\n",
        "        }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D4CRI_GB-btq",
        "outputId": "ee0ac2f7-0316-4a41-fdf4-728b32c3fb8e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "--- Running with RAG strategy ---\n",
            "\n",
            "====================\n",
            "DEBUG: Contextual Compression Retriever\n",
            "====================\n",
            "'Initialized and ready.'\n",
            "\n",
            "====================\n",
            "DEBUG: Using Prompt Strategy: permissive_context\n",
            "====================\n",
            "('You are a helpful expert assistant for Texas Instruments products.\\n'\n",
            " \"            Answer the user's question based on the context provided.\\n\"\n",
            " '            If the context is not sufficient to answer the question, use '\n",
            " 'your own knowledge to provide a comprehensive answer,\\n'\n",
            " '            but you must state that the information comes from your general '\n",
            " 'knowledge.\\n'\n",
            " '            \\n'\n",
            " '            Context: {context}\\n'\n",
            " '            Question: {question}\\n'\n",
            " '            ')\n",
            "\n",
            "====================\n",
            "DEBUG: Retrieved Documents (Simple)\n",
            "====================\n",
            "[Document(metadata={'id': 'pdf_doc_1_chunk_13_da6ca75b041245d18674dd726a03d374'}, page_content='the mmWave 14xx device). The accelerator is connected to a 128-bit bus that is present in the main\\nprocessor system, as shown in Figure 1.\\nThe Radar Hardware Accelerator module comprises an accelerator engine and four memories, each of\\n16KB size, which are used to send input data to and pull output data from the accelerator engine. These\\nmemories are referred to as local memories of the Radar Accelerator (ACCEL_MEM). For convenience,\\nthese four local memories are referred to as ACCEL_MEM0, ACCEL_MEM1, ACCEL_MEM2, and\\nACCEL_MEM3.\\nFigure 1. Radar Hardware Accelerator (mmWave 14xx Device)www.ti.com Radar Hardware Accelerator  Overview\\n5SWRU526 May 2017\\nSubmit Documentation Feedback\\nCopyright  2017, Texas Instruments Incorporated\\nRadar Hardware Accelerator - Part 1\\n1.3.1 High-Level Data Flow\\nThe typical data flow is that the DMA module is used to bring samples (for example, FFT input samples)'),\n",
            " Document(metadata={'id': 'pdf_doc_1_chunk_17_a063124a8ac7444e8ff0f5ccf5a1ed85'}, page_content='STATERRCODE register description in Table 3.\\nThe Radar Hardware Accelerator and the main processor (Cortex-R4F) in the mmWave 14xx device\\noperate on a single clock domain and the operating clock frequency is 200 MHz.\\nThe accelerator local memories are 128-bits wide, for example, each of the 16KB banks is implemented\\nas 1024 words of 128 bits each. This allows the DMA to bring data into the accelerator local memories\\nefficiently (up to a maximum throughput of 128 bits per clock cycle, depending upon the DMA\\nconfiguration).\\nIt is important to note that any of the four local memories can be the source of the input samples to the\\naccelerator engine and any of the four local memories can be the destination for the output samples from\\nthe accelerator engine  with the important restriction that the source and destination memories cannot be\\nthe same 16KB bank. Note also that the accelerator local memories do not necessarily need to be used in'),\n",
            " Document(metadata={'id': 'pdf_doc_1_chunk_15_9280380379c945fdbfce32b0fe5183f1'}, page_content='the DMA and written back into the Radar data memory for further processing by the main processor.\\nNote that in the mmWave 14xx device, the Radar Hardware Accelerator is included as part of a single\\nchip along with the mmWave RF and analog front end. In this device, two of the accelerator local\\nmemories, namely ACCEL_MEM0 and ACCEL_MEM1, are directly shared with the ping and pong ADC\\nbuffers (which are 16KB each)  such that the ADC output samples for first-dimension FFT processing are\\ndirectly and immediately available to the Radar Hardware Accelerator at the end of each chirp, without\\nneeding a DMA transfer. After the first-dimension FFT processing is complete (typically, at the end of the\\nactive transmission of chirps in a frame), it is possible to freely use these memories for second-dimension\\nFFT processing by bringing in data to these memories through DMA transfer.\\nThe purpose behind the four separate local memories (16KB each) inside the Radar Hardware Accelerator'),\n",
            " Document(metadata={'id': 'pdf_doc_1_chunk_40_0d650815cc3a42b892ebe88f9a82d019'}, page_content='end of every chirp or at the end of every few chirps or at the end of every specified number of ADC\\nsamples. This mmWave 14xx digital front-end configuration is accomplished using other registers\\nunrelated to the Radar Hardware Accelerator and not described in this document.www.ti.com Accelerator Engine  State Machine\\n11SWRU526 May 2017\\nSubmit Documentation Feedback\\nCopyright  2017, Texas Instruments Incorporated\\nRadar Hardware Accelerator - Part 1\\nNow, using this trigger mode (TRIGMODE = 010b) allows the accelerator computations to start\\nwhenever the ping-to-pong or pong-to-ping switch happens in the ADC buffer, thus enabling inline per-\\nchirp processing. It is important to mention here that the user must take care to ensure that processing\\nof the current ping data is completed by the accelerator, before the next switch/trigger happens on the\\nADC buffer. In other words, the chirp duration (ping-pong switch frequency) must be configured to be'),\n",
            " Document(metadata={'id': 'pdf_doc_1_chunk_51_d191c9094fe0456abdc12d45cea7588e'}, page_content='through which the state machine loops through. The state machine starts at the\\nparameter set specified by PARAMSTART and loads each parameter set one after\\nanother and runs the accelerator as per that configuration. When the state machine\\nreaches the parameter set specified by PARAMSTOP, it loops back to the start\\nindex as specified by PARAMSTART.\\nPARAMSTOP 4 No\\nFFT1DEN 1 No\\nADC buffer sharing mode (mmWave 14xx):\\nThis register is relevant in mmWave 14xx, where the Radar Hardware Accelerator is\\nincluded in a single device along with the mmWave RF front-end. In such a case,\\nduring active chirp transmission and inline first dimension FFT processing, the\\nACCEL_MEM0 and ACCEL_MEM1 memories of the accelerator are shared as ping-\\npong ADC buffers. This register bit needs to be set during this time, so that while the\\ndigital front end writes ADC samples to the ping buffer, the accelerator automatically\\naccesses (only) the pong buffer, and vice versa. At the end of the active'),\n",
            " Document(metadata={'id': 'pdf_doc_1_chunk_64_72f04cac09704a1daaf973f8bdc1dde5'}, page_content='they can be read as real samples or complex samples. These two aspects are configured using register\\nbits SRC16b32b and SRCREAL. See Table 2 for a description of these and other registers pertaining to\\nthe input formatter block. As an example, if SRC16b32b = 0 and SRCREAL = 0, then the input samples\\nare read from the memory as 16-bit complex samples (16-bit I and 16-bit Q), shown in Figure 6. In the\\nmmWave 14xx device, the ADC buffer is always filled with complex samples from the digital front end \\nthis is true even if the device is configured for real-only operation, in which case the Q-channel output is\\nwritten with zero values. Therefore, for all purposes of part one of the user guide, SRCREAL can be\\nconfigured as 0.\\nAn important feature of the input formatter block is that it supports flexible access pattern to fetch data\\nfrom the source memory, which makes it convenient when the data corresponding to multiple RX channels'),\n",
            " Document(metadata={'id': 'pdf_doc_1_chunk_39_5a0077fef314477dac4cd5173546f6f2'}, page_content='the ADC ping and pong buffers are shared with the accelerator local memories (ACCEL_MEM0 and\\nACCEL_MEM1), such that the ADC data is directly available to the accelerator for processing during\\nactive chirping portion of the frame. This sharing mode is enabled by setting the FFT1DEN register bit\\nbefore the start of the frame. In this trigger mode, the state machine of the accelerator starts the\\ncomputations for the current parameter set as soon as the ADC buffer switches from ping-to-pong or\\npong-to-ping. As an example, during the active chirping portion of a frame, the mmWave 14xx digital\\nfront end and ADC buffer can be configured to switch from ping-to-pong or pong-to-ping buffer at the\\nend of every chirp or at the end of every few chirps or at the end of every specified number of ADC\\nsamples. This mmWave 14xx digital front-end configuration is accomplished using other registers'),\n",
            " Document(metadata={'id': 'pdf_doc_1_chunk_38_7f8a7bb770fe4445baef375c726d9252'}, page_content='computations. In this trigger mode, the state machine waits for a software-based trigger, which involves\\nthe main processor setting a separate self-clearing bit in a CR42ACCTRIG register (single-bit register).\\nThe state machine keeps monitoring that register bit and waits as long as the value is zero. When the\\nvalue becomes 1 (set), the state machine gets triggered to start the accelerator operations for the\\ncurrent parameter set.\\n Wait for the ADC buffer ping-to-pong or pong-to-ping switch (TRIGMODE = 010b): This trigger mode is\\nspecific to the mmWave 14xx device, which has RF and analog front end integrated in the same chip\\nwith the main processor and the Radar Hardware Accelerator. Recall that in the mmWave 14xx device,\\nthe ADC ping and pong buffers are shared with the accelerator local memories (ACCEL_MEM0 and\\nACCEL_MEM1), such that the ADC data is directly available to the accelerator for processing during'),\n",
            " Document(metadata={'id': 'pdf_doc_1_chunk_3_923b12fcbfa744f08854745a5f943067'}, page_content='List of Figures\\n1 Radar Hardware Accelerator (mmWave 14xx Device) ................................................................. 4\\n2 Accelerator Engine Block Diagram ........................................................................................ 6\\n3 Parameter-Set Configuration Memory (512 Bytes)...................................................................... 7\\n4 State Machine................................................................................................................ 9\\n5 Input Formatter ............................................................................................................. 15\\n6 Input Formatter Source Memory Access Pattern (Example) ......................................................... 17\\n7 Invalid Configuration Example ........................................................................................... 18'),\n",
            " Document(metadata={'id': 'pdf_doc_1_chunk_10_cd57b3cc0f1d4d0aad61dc9205171ec3'}, page_content='radar signal processing can be done within the Radar Hardware Accelerator, while still retaining the\\nflexibility of implementing other proprietary algorithms in the main processor.\\n1.2 Key Features\\nThe main features of the Radar Hardware Accelerator are as follows.\\n Fast FFT computation, with programmable FFT sizes (powers of 2) up to 1024-pt complex FFT\\n Internal FFT bit width of 24 bits (for each I and Q) for good SQNR performance, with fully\\nprogrammable butterfly scaling at every radix-2 stage for user flexibility\\n Built-in capabilities for simple pre-FFT processing  specifically, programmable windowing, basic\\ninterference zeroing-out, and basic BPM removal\\n Magnitude (absolute value) and log-magnitude computation capability\\n Flexible data flow and data sample arrangement to support efficient multidimensional FFT operations\\nand transpose accesses as required\\n Chaining and looping mechanism to sequence a set of accelerator operations one-after-another with')]\n",
            "\n",
            "====================\n",
            "DEBUG: Documents going INTO Re-ranker (10 docs)\n",
            "====================\n",
            "[Document(metadata={'id': 'pdf_doc_1_chunk_13_da6ca75b041245d18674dd726a03d374'}, page_content='the mmWave 14xx device). The accelerator is connected to a 128-bit bus that is present in the main\\nprocessor system, as shown in Figure 1.\\nThe Radar Hardware Accelerator module comprises an accelerator engine and four memories, each of\\n16KB size, which are used to send input data to and pull output data from the accelerator engine. These\\nmemories are referred to as local memories of the Radar Accelerator (ACCEL_MEM). For convenience,\\nthese four local memories are referred to as ACCEL_MEM0, ACCEL_MEM1, ACCEL_MEM2, and\\nACCEL_MEM3.\\nFigure 1. Radar Hardware Accelerator (mmWave 14xx Device)www.ti.com Radar Hardware Accelerator  Overview\\n5SWRU526 May 2017\\nSubmit Documentation Feedback\\nCopyright  2017, Texas Instruments Incorporated\\nRadar Hardware Accelerator - Part 1\\n1.3.1 High-Level Data Flow\\nThe typical data flow is that the DMA module is used to bring samples (for example, FFT input samples)'),\n",
            " Document(metadata={'id': 'pdf_doc_1_chunk_17_a063124a8ac7444e8ff0f5ccf5a1ed85'}, page_content='STATERRCODE register description in Table 3.\\nThe Radar Hardware Accelerator and the main processor (Cortex-R4F) in the mmWave 14xx device\\noperate on a single clock domain and the operating clock frequency is 200 MHz.\\nThe accelerator local memories are 128-bits wide, for example, each of the 16KB banks is implemented\\nas 1024 words of 128 bits each. This allows the DMA to bring data into the accelerator local memories\\nefficiently (up to a maximum throughput of 128 bits per clock cycle, depending upon the DMA\\nconfiguration).\\nIt is important to note that any of the four local memories can be the source of the input samples to the\\naccelerator engine and any of the four local memories can be the destination for the output samples from\\nthe accelerator engine  with the important restriction that the source and destination memories cannot be\\nthe same 16KB bank. Note also that the accelerator local memories do not necessarily need to be used in'),\n",
            " Document(metadata={'id': 'pdf_doc_1_chunk_15_9280380379c945fdbfce32b0fe5183f1'}, page_content='the DMA and written back into the Radar data memory for further processing by the main processor.\\nNote that in the mmWave 14xx device, the Radar Hardware Accelerator is included as part of a single\\nchip along with the mmWave RF and analog front end. In this device, two of the accelerator local\\nmemories, namely ACCEL_MEM0 and ACCEL_MEM1, are directly shared with the ping and pong ADC\\nbuffers (which are 16KB each)  such that the ADC output samples for first-dimension FFT processing are\\ndirectly and immediately available to the Radar Hardware Accelerator at the end of each chirp, without\\nneeding a DMA transfer. After the first-dimension FFT processing is complete (typically, at the end of the\\nactive transmission of chirps in a frame), it is possible to freely use these memories for second-dimension\\nFFT processing by bringing in data to these memories through DMA transfer.\\nThe purpose behind the four separate local memories (16KB each) inside the Radar Hardware Accelerator'),\n",
            " Document(metadata={'id': 'pdf_doc_1_chunk_40_0d650815cc3a42b892ebe88f9a82d019'}, page_content='end of every chirp or at the end of every few chirps or at the end of every specified number of ADC\\nsamples. This mmWave 14xx digital front-end configuration is accomplished using other registers\\nunrelated to the Radar Hardware Accelerator and not described in this document.www.ti.com Accelerator Engine  State Machine\\n11SWRU526 May 2017\\nSubmit Documentation Feedback\\nCopyright  2017, Texas Instruments Incorporated\\nRadar Hardware Accelerator - Part 1\\nNow, using this trigger mode (TRIGMODE = 010b) allows the accelerator computations to start\\nwhenever the ping-to-pong or pong-to-ping switch happens in the ADC buffer, thus enabling inline per-\\nchirp processing. It is important to mention here that the user must take care to ensure that processing\\nof the current ping data is completed by the accelerator, before the next switch/trigger happens on the\\nADC buffer. In other words, the chirp duration (ping-pong switch frequency) must be configured to be'),\n",
            " Document(metadata={'id': 'pdf_doc_1_chunk_51_d191c9094fe0456abdc12d45cea7588e'}, page_content='through which the state machine loops through. The state machine starts at the\\nparameter set specified by PARAMSTART and loads each parameter set one after\\nanother and runs the accelerator as per that configuration. When the state machine\\nreaches the parameter set specified by PARAMSTOP, it loops back to the start\\nindex as specified by PARAMSTART.\\nPARAMSTOP 4 No\\nFFT1DEN 1 No\\nADC buffer sharing mode (mmWave 14xx):\\nThis register is relevant in mmWave 14xx, where the Radar Hardware Accelerator is\\nincluded in a single device along with the mmWave RF front-end. In such a case,\\nduring active chirp transmission and inline first dimension FFT processing, the\\nACCEL_MEM0 and ACCEL_MEM1 memories of the accelerator are shared as ping-\\npong ADC buffers. This register bit needs to be set during this time, so that while the\\ndigital front end writes ADC samples to the ping buffer, the accelerator automatically\\naccesses (only) the pong buffer, and vice versa. At the end of the active'),\n",
            " Document(metadata={'id': 'pdf_doc_1_chunk_64_72f04cac09704a1daaf973f8bdc1dde5'}, page_content='they can be read as real samples or complex samples. These two aspects are configured using register\\nbits SRC16b32b and SRCREAL. See Table 2 for a description of these and other registers pertaining to\\nthe input formatter block. As an example, if SRC16b32b = 0 and SRCREAL = 0, then the input samples\\nare read from the memory as 16-bit complex samples (16-bit I and 16-bit Q), shown in Figure 6. In the\\nmmWave 14xx device, the ADC buffer is always filled with complex samples from the digital front end \\nthis is true even if the device is configured for real-only operation, in which case the Q-channel output is\\nwritten with zero values. Therefore, for all purposes of part one of the user guide, SRCREAL can be\\nconfigured as 0.\\nAn important feature of the input formatter block is that it supports flexible access pattern to fetch data\\nfrom the source memory, which makes it convenient when the data corresponding to multiple RX channels'),\n",
            " Document(metadata={'id': 'pdf_doc_1_chunk_39_5a0077fef314477dac4cd5173546f6f2'}, page_content='the ADC ping and pong buffers are shared with the accelerator local memories (ACCEL_MEM0 and\\nACCEL_MEM1), such that the ADC data is directly available to the accelerator for processing during\\nactive chirping portion of the frame. This sharing mode is enabled by setting the FFT1DEN register bit\\nbefore the start of the frame. In this trigger mode, the state machine of the accelerator starts the\\ncomputations for the current parameter set as soon as the ADC buffer switches from ping-to-pong or\\npong-to-ping. As an example, during the active chirping portion of a frame, the mmWave 14xx digital\\nfront end and ADC buffer can be configured to switch from ping-to-pong or pong-to-ping buffer at the\\nend of every chirp or at the end of every few chirps or at the end of every specified number of ADC\\nsamples. This mmWave 14xx digital front-end configuration is accomplished using other registers'),\n",
            " Document(metadata={'id': 'pdf_doc_1_chunk_38_7f8a7bb770fe4445baef375c726d9252'}, page_content='computations. In this trigger mode, the state machine waits for a software-based trigger, which involves\\nthe main processor setting a separate self-clearing bit in a CR42ACCTRIG register (single-bit register).\\nThe state machine keeps monitoring that register bit and waits as long as the value is zero. When the\\nvalue becomes 1 (set), the state machine gets triggered to start the accelerator operations for the\\ncurrent parameter set.\\n Wait for the ADC buffer ping-to-pong or pong-to-ping switch (TRIGMODE = 010b): This trigger mode is\\nspecific to the mmWave 14xx device, which has RF and analog front end integrated in the same chip\\nwith the main processor and the Radar Hardware Accelerator. Recall that in the mmWave 14xx device,\\nthe ADC ping and pong buffers are shared with the accelerator local memories (ACCEL_MEM0 and\\nACCEL_MEM1), such that the ADC data is directly available to the accelerator for processing during'),\n",
            " Document(metadata={'id': 'pdf_doc_1_chunk_3_923b12fcbfa744f08854745a5f943067'}, page_content='List of Figures\\n1 Radar Hardware Accelerator (mmWave 14xx Device) ................................................................. 4\\n2 Accelerator Engine Block Diagram ........................................................................................ 6\\n3 Parameter-Set Configuration Memory (512 Bytes)...................................................................... 7\\n4 State Machine................................................................................................................ 9\\n5 Input Formatter ............................................................................................................. 15\\n6 Input Formatter Source Memory Access Pattern (Example) ......................................................... 17\\n7 Invalid Configuration Example ........................................................................................... 18'),\n",
            " Document(metadata={'id': 'pdf_doc_1_chunk_10_cd57b3cc0f1d4d0aad61dc9205171ec3'}, page_content='radar signal processing can be done within the Radar Hardware Accelerator, while still retaining the\\nflexibility of implementing other proprietary algorithms in the main processor.\\n1.2 Key Features\\nThe main features of the Radar Hardware Accelerator are as follows.\\n Fast FFT computation, with programmable FFT sizes (powers of 2) up to 1024-pt complex FFT\\n Internal FFT bit width of 24 bits (for each I and Q) for good SQNR performance, with fully\\nprogrammable butterfly scaling at every radix-2 stage for user flexibility\\n Built-in capabilities for simple pre-FFT processing  specifically, programmable windowing, basic\\ninterference zeroing-out, and basic BPM removal\\n Magnitude (absolute value) and log-magnitude computation capability\\n Flexible data flow and data sample arrangement to support efficient multidimensional FFT operations\\nand transpose accesses as required\\n Chaining and looping mechanism to sequence a set of accelerator operations one-after-another with')]\n",
            "\n",
            "====================\n",
            "DEBUG: Documents COMING OUT of Re-ranker (5 docs)\n",
            "====================\n",
            "[Document(metadata={'id': 'pdf_doc_1_chunk_13_da6ca75b041245d18674dd726a03d374'}, page_content='the mmWave 14xx device). The accelerator is connected to a 128-bit bus that is present in the main\\nprocessor system, as shown in Figure 1.\\nThe Radar Hardware Accelerator module comprises an accelerator engine and four memories, each of\\n16KB size, which are used to send input data to and pull output data from the accelerator engine. These\\nmemories are referred to as local memories of the Radar Accelerator (ACCEL_MEM). For convenience,\\nthese four local memories are referred to as ACCEL_MEM0, ACCEL_MEM1, ACCEL_MEM2, and\\nACCEL_MEM3.\\nFigure 1. Radar Hardware Accelerator (mmWave 14xx Device)www.ti.com Radar Hardware Accelerator  Overview\\n5SWRU526 May 2017\\nSubmit Documentation Feedback\\nCopyright  2017, Texas Instruments Incorporated\\nRadar Hardware Accelerator - Part 1\\n1.3.1 High-Level Data Flow\\nThe typical data flow is that the DMA module is used to bring samples (for example, FFT input samples)'),\n",
            " Document(metadata={'id': 'pdf_doc_1_chunk_17_a063124a8ac7444e8ff0f5ccf5a1ed85'}, page_content='STATERRCODE register description in Table 3.\\nThe Radar Hardware Accelerator and the main processor (Cortex-R4F) in the mmWave 14xx device\\noperate on a single clock domain and the operating clock frequency is 200 MHz.\\nThe accelerator local memories are 128-bits wide, for example, each of the 16KB banks is implemented\\nas 1024 words of 128 bits each. This allows the DMA to bring data into the accelerator local memories\\nefficiently (up to a maximum throughput of 128 bits per clock cycle, depending upon the DMA\\nconfiguration).\\nIt is important to note that any of the four local memories can be the source of the input samples to the\\naccelerator engine and any of the four local memories can be the destination for the output samples from\\nthe accelerator engine  with the important restriction that the source and destination memories cannot be\\nthe same 16KB bank. Note also that the accelerator local memories do not necessarily need to be used in'),\n",
            " Document(metadata={'id': 'pdf_doc_1_chunk_15_9280380379c945fdbfce32b0fe5183f1'}, page_content='the DMA and written back into the Radar data memory for further processing by the main processor.\\nNote that in the mmWave 14xx device, the Radar Hardware Accelerator is included as part of a single\\nchip along with the mmWave RF and analog front end. In this device, two of the accelerator local\\nmemories, namely ACCEL_MEM0 and ACCEL_MEM1, are directly shared with the ping and pong ADC\\nbuffers (which are 16KB each)  such that the ADC output samples for first-dimension FFT processing are\\ndirectly and immediately available to the Radar Hardware Accelerator at the end of each chirp, without\\nneeding a DMA transfer. After the first-dimension FFT processing is complete (typically, at the end of the\\nactive transmission of chirps in a frame), it is possible to freely use these memories for second-dimension\\nFFT processing by bringing in data to these memories through DMA transfer.\\nThe purpose behind the four separate local memories (16KB each) inside the Radar Hardware Accelerator'),\n",
            " Document(metadata={'id': 'pdf_doc_1_chunk_64_72f04cac09704a1daaf973f8bdc1dde5'}, page_content='they can be read as real samples or complex samples. These two aspects are configured using register\\nbits SRC16b32b and SRCREAL. See Table 2 for a description of these and other registers pertaining to\\nthe input formatter block. As an example, if SRC16b32b = 0 and SRCREAL = 0, then the input samples\\nare read from the memory as 16-bit complex samples (16-bit I and 16-bit Q), shown in Figure 6. In the\\nmmWave 14xx device, the ADC buffer is always filled with complex samples from the digital front end \\nthis is true even if the device is configured for real-only operation, in which case the Q-channel output is\\nwritten with zero values. Therefore, for all purposes of part one of the user guide, SRCREAL can be\\nconfigured as 0.\\nAn important feature of the input formatter block is that it supports flexible access pattern to fetch data\\nfrom the source memory, which makes it convenient when the data corresponding to multiple RX channels'),\n",
            " Document(metadata={'id': 'pdf_doc_1_chunk_38_7f8a7bb770fe4445baef375c726d9252'}, page_content='computations. In this trigger mode, the state machine waits for a software-based trigger, which involves\\nthe main processor setting a separate self-clearing bit in a CR42ACCTRIG register (single-bit register).\\nThe state machine keeps monitoring that register bit and waits as long as the value is zero. When the\\nvalue becomes 1 (set), the state machine gets triggered to start the accelerator operations for the\\ncurrent parameter set.\\n Wait for the ADC buffer ping-to-pong or pong-to-ping switch (TRIGMODE = 010b): This trigger mode is\\nspecific to the mmWave 14xx device, which has RF and analog front end integrated in the same chip\\nwith the main processor and the Radar Hardware Accelerator. Recall that in the mmWave 14xx device,\\nthe ADC ping and pong buffers are shared with the accelerator local memories (ACCEL_MEM0 and\\nACCEL_MEM1), such that the ADC data is directly available to the accelerator for processing during')]\n",
            "\n",
            "--- FINAL OUTPUT ---\n",
            "{'answer': 'The key features of the mmWave 14xx devices include:\\n'\n",
            "           '\\n'\n",
            "           '1. **Integrated Radar Hardware Accelerator**: The device includes '\n",
            "           \"a dedicated Radar Hardware Accelerator that modifies the chip's \"\n",
            "           'performance for processing radar signals efficiently. It operates '\n",
            "           'on a single clock domain with a frequency of 200 MHz.\\n'\n",
            "           '\\n'\n",
            "           '2. **Local Memory Configuration**: The Radar Hardware Accelerator '\n",
            "           'is equipped with four local memories, each 16KB in size and '\n",
            "           '128-bits wide. These memories (ACCEL_MEM0, ACCEL_MEM1, ACCEL_MEM2, '\n",
            "           'ACCEL_MEM3) allow for efficient data transfer and processing '\n",
            "           'directly related to radar signal processing.\\n'\n",
            "           '\\n'\n",
            "           '3. **Shared ADC Buffers**: ACCEL_MEM0 and ACCEL_MEM1 are directly '\n",
            "           'shared with the ping and pong ADC buffers, enabling immediate '\n",
            "           'access to ADC output samples for first-dimension FFT processing '\n",
            "           'without additional DMA transfer overhead.\\n'\n",
            "           '\\n'\n",
            "           '4. **Flexible Input Formatting**: The input formatter block '\n",
            "           'supports flexible access patterns to fetch data from source '\n",
            "           'memories, making it convenient for handling data from multiple RX '\n",
            "           'channels, which is vital for radar applications.\\n'\n",
            "           '\\n'\n",
            "           '5. **Complex Sample Handling**: The ADC buffers are filled with '\n",
            "           'complex samples, even in configurations for real-only operations. '\n",
            "           'The Q-channel output is set to zero in those cases, ensuring that '\n",
            "           'the device can handle complex data effectively.\\n'\n",
            "           '\\n'\n",
            "           '6. **High Throughput**: The architecture allows for up to 128 bits '\n",
            "           'of data to be processed per clock cycle, facilitating high data '\n",
            "           'throughput depending on DMA configuration.\\n'\n",
            "           '\\n'\n",
            "           '7. **Trigger Modes**: The device features specific trigger modes '\n",
            "           '(e.g., TRIGMODE = 010b) that manage ADC buffer operations, such as '\n",
            "           'switching between ping and pong buffers, which is crucial for '\n",
            "           'efficient data processing during radar operations.\\n'\n",
            "           '\\n'\n",
            "           'These features make the mmWave 14xx devices suitable for advanced '\n",
            "           'radar applications in various fields such as automotive, '\n",
            "           'industrial, and consumer electronics.',\n",
            " 'context': [Document(metadata={'id': 'pdf_doc_1_chunk_13_da6ca75b041245d18674dd726a03d374'}, page_content='the mmWave 14xx device). The Radar Hardware Accelerator module comprises an accelerator engine and four memories, each of 16KB size, which are used to send input data to and pull output data from the accelerator engine. These memories are referred to as local memories of the Radar Accelerator (ACCEL_MEM). For convenience, these four local memories are referred to as ACCEL_MEM0, ACCEL_MEM1, ACCEL_MEM2, and ACCEL_MEM3.'),\n",
            "             Document(metadata={'id': 'pdf_doc_1_chunk_17_a063124a8ac7444e8ff0f5ccf5a1ed85'}, page_content='The Radar Hardware Accelerator and the main processor (Cortex-R4F) in the mmWave 14xx device operate on a single clock domain and the operating clock frequency is 200 MHz. The accelerator local memories are 128-bits wide, for example, each of the 16KB banks is implemented as 1024 words of 128 bits each. This allows the DMA to bring data into the accelerator local memories efficiently (up to a maximum throughput of 128 bits per clock cycle, depending upon the DMA configuration). It is important to note that any of the four local memories can be the source of the input samples to the accelerator engine and any of the four local memories can be the destination for the output samples from the accelerator engine  with the important restriction that the source and destination memories cannot be the same 16KB bank.'),\n",
            "             Document(metadata={'id': 'pdf_doc_1_chunk_15_9280380379c945fdbfce32b0fe5183f1'}, page_content='the mmWave 14xx device, the Radar Hardware Accelerator is included as part of a single chip along with the mmWave RF and analog front end. In this device, two of the accelerator local memories, namely ACCEL_MEM0 and ACCEL_MEM1, are directly shared with the ping and pong ADC buffers (which are 16KB each)  such that the ADC output samples for first-dimension FFT processing are directly and immediately available to the Radar Hardware Accelerator at the end of each chirp, without needing a DMA transfer. After the first-dimension FFT processing is complete (typically, at the end of the active transmission of chirps in a frame), it is possible to freely use these memories for second-dimension FFT processing by bringing in data to these memories through DMA transfer. The purpose behind the four separate local memories (16KB each) inside the Radar Hardware Accelerator'),\n",
            "             Document(metadata={'id': 'pdf_doc_1_chunk_64_72f04cac09704a1daaf973f8bdc1dde5'}, page_content='they can be read as real samples or complex samples. These two aspects are configured using register bits SRC16b32b and SRCREAL. In the mmWave 14xx device, the ADC buffer is always filled with complex samples from the digital front end  this is true even if the device is configured for real-only operation, in which case the Q-channel output is written with zero values. An important feature of the input formatter block is that it supports flexible access pattern to fetch data from the source memory, which makes it convenient when the data corresponding to multiple RX channels.'),\n",
            "             Document(metadata={'id': 'pdf_doc_1_chunk_38_7f8a7bb770fe4445baef375c726d9252'}, page_content='Wait for the ADC buffer ping-to-pong or pong-to-ping switch (TRIGMODE = 010b): This trigger mode is specific to the mmWave 14xx device, which has RF and analog front end integrated in the same chip with the main processor and the Radar Hardware Accelerator. Recall that in the mmWave 14xx device, the ADC ping and pong buffers are shared with the accelerator local memories (ACCEL_MEM0 and ACCEL_MEM1), such that the ADC data is directly available to the accelerator for processing during')],\n",
            " 'question': 'What are the key features of the 14xx mmWave devices?',\n",
            " 'strategy': 'simple RAG + semantic_re_ranking+contextual_compression '\n",
            "             'Post-Retrieval Processing + permissive_context Prompt Strategy'}\n",
            "\n",
            "\n",
            "--- Running with LLM Only Strategy ---\n",
            "\n",
            "--- FINAL OUTPUT ---\n",
            "{'answer': 'The 14xx mmWave devices, typically referring to a range of '\n",
            "           'millimeter-wave technology products operating around the frequency '\n",
            "           'of 14 GHz, are used primarily in advanced wireless communication '\n",
            "           'systems such as 5G networks, satellite communications, and various '\n",
            "           'radar applications. While specific features may vary across '\n",
            "           'different manufacturers and models, key features often associated '\n",
            "           'with this category include:\\n'\n",
            "           '\\n'\n",
            "           '1. **High Frequency Operation**: Operating in the millimeter-wave '\n",
            "           'frequency range (typically around 14 GHz), these devices can '\n",
            "           'achieve significant bandwidth, allowing for high data rates.\\n'\n",
            "           '\\n'\n",
            "           '2. **Wide Bandwidth**: mmWave devices often support wider '\n",
            "           'channels, enabling faster data transmission and increased capacity '\n",
            "           'for multiple users.\\n'\n",
            "           '\\n'\n",
            "           '3. **Low Latency**: The ability to transmit data at high '\n",
            "           'frequencies allows for lower latency communication, which is '\n",
            "           'essential for real-time applications such as autonomous driving '\n",
            "           'and augmented reality.\\n'\n",
            "           '\\n'\n",
            "           '4. **Advanced Modulation Techniques**: These devices support '\n",
            "           'various advanced modulation schemes (like QAM) for efficient data '\n",
            "           'transmission, enabling more bits to be transmitted per symbol.\\n'\n",
            "           '\\n'\n",
            "           '5. **Higher Density**: The small physical size of components '\n",
            "           'operating at mmWave frequencies allows for higher integration '\n",
            "           'density. This results in reduced size and weight for mobile '\n",
            "           'devices and infrastructure equipment.\\n'\n",
            "           '\\n'\n",
            "           '6. **Beamforming Capabilities**: Many 14xx mmWave devices feature '\n",
            "           'advanced beamforming technologies that improve signal quality, '\n",
            "           'extend range, and enhance user experience by directing signals '\n",
            "           'towards specific users.\\n'\n",
            "           '\\n'\n",
            "           '7. **Multi-Input Multi-Output (MIMO)**: To further increase '\n",
            "           'capacity and spectral efficiency, many devices incorporate MIMO '\n",
            "           'technology, enabling simultaneous data streams to and from '\n",
            "           'multiple antennas.\\n'\n",
            "           '\\n'\n",
            "           '8. **Interference Mitigation**: These devices often include '\n",
            "           'features to minimize interference and improve signal robustness, '\n",
            "           'such as adaptive coding and modulation.\\n'\n",
            "           '\\n'\n",
            "           '9. **Multi-band Operation**: Some 14xx mmWave devices are designed '\n",
            "           'to operate across multiple frequency bands, increasing flexibility '\n",
            "           'and compatibility with various communication standards.\\n'\n",
            "           '\\n'\n",
            "           '10. **Integration with Legacy Systems**: They may offer '\n",
            "           'compatibility with existing communication infrastructures, '\n",
            "           'allowing for smoother transitions from older technologies.\\n'\n",
            "           '\\n'\n",
            "           '11. **Robustness to Environmental Factors**: While mmWave '\n",
            "           'frequencies can be susceptible to atmospheric absorption and '\n",
            "           'physical obstructions, many devices come with technologies to '\n",
            "           'mitigate these challenges.\\n'\n",
            "           '\\n'\n",
            "           '12. **Power Efficiency**: Efforts are often made to improve power '\n",
            "           'efficiency in these devices, critical for battery-operated and '\n",
            "           'portable technologies.\\n'\n",
            "           '\\n'\n",
            "           '13. **Compliance and Standards**: Many devices are designed to '\n",
            "           'comply with international standards for communication, ensuring '\n",
            "           'interoperability in global markets.\\n'\n",
            "           '\\n'\n",
            "           'These features make 14xx mmWave devices vital components in the '\n",
            "           'advancement of wireless technologies, supporting the growing '\n",
            "           'demand for high-speed data services and enhanced communication '\n",
            "           'capabilities.',\n",
            " 'context': 'N/A',\n",
            " 'question': 'What are the key features of the 14xx mmWave devices?',\n",
            " 'strategy': 'llm_only'}\n"
          ]
        }
      ],
      "source": [
        "# --- Example Usage ---\n",
        "# main.py\n",
        "from pprint import pprint\n",
        "\n",
        "\n",
        "# Get API keys using using dotenv\n",
        "load_dotenv()\n",
        "PINECONE_API_KEY = os.getenv(\"PINECONE_API_KEY_SHIVAM\")\n",
        "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
        "\n",
        "# Get API keys using google.colab\n",
        "# from google.colab import userdata\n",
        "# OPENAI_API_KEY = userdata.get('OPENAI_API_KEY')\n",
        "# PINECONE_API_KEY = userdata.get('PINECONE_API_KEY_SHIVAM')\n",
        "\n",
        "\n",
        "# --- Example 1: RAG Fusion with Debugging Enabled ---\n",
        "print(\"\\n\\n\" + \"--- Running with RAG strategy ---\")\n",
        "config = {\n",
        "    \"llm_model\": \"gpt-4o-mini\",\n",
        "    \"retrieval_strategy\": \"decomposition\",\n",
        "    \"post_retrieval_processing\": \"semantic_re_ranking+contextual_compression\",\n",
        "    \"prompt_strategy\": \"permissive_context\",\n",
        "    \"index_name\": \"swru526-pine\",\n",
        "    \"namespace\": \"example-namespace\",\n",
        "    \"top_k\": 10,\n",
        "    \"reranker_top_n\": 5,  # This is for the re-ranker, if used\n",
        "    \"debug\": True  # <-- Enable debugging\n",
        "}\n",
        "\n",
        "orchestrator_RAG = RAGOrchestrator(config)\n",
        "question = \"What are the key features of the 14xx mmWave devices?\"\n",
        "result_RAG = orchestrator_RAG.invoke(question)\n",
        "print(\"\\n--- FINAL OUTPUT ---\")\n",
        "pprint(result_RAG)\n",
        "\n",
        "\n",
        "# --- Example 2: LLM Only (No RAG) via Config ---\n",
        "print(\"\\n\\n\" + \"--- Running with LLM Only Strategy ---\")\n",
        "llm_only_config = {\n",
        "    \"llm_model\": \"gpt-4o-mini\",\n",
        "    \"retrieval_strategy\": \"llm_only\",\n",
        "    \"debug\": False # Debug flag works here too\n",
        "}\n",
        "\n",
        "orchestrator_llm_only = RAGOrchestrator(llm_only_config)\n",
        "result_llm_only = orchestrator_llm_only.invoke(question)\n",
        "print(\"\\n--- FINAL OUTPUT ---\")\n",
        "pprint(result_llm_only)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KXmKoJU9jJqj"
      },
      "outputs": [],
      "source": [
        "# RAG Orchestrator Configuration Guide\n",
        "\n",
        "# 1. Core Retrieval Strategy\n",
        "# This is the main method used to find and fetch the initial set of documents.\n",
        "# \"retrieval_strategy\":\n",
        "#     \"simple\": A single vector search against the user's query.\n",
        "#     \"multi_query\": Generates multiple variations of the query and combines the results.\n",
        "#     \"rag_fusion\": Generates multiple variations and combines results using Reciprocal Rank Fusion.\n",
        "#     \"decomposition\": Breaks a complex query into sub-questions and retrieves for each.\n",
        "#     \"step_back\": Asks a more general question to get broader context.\n",
        "#     \"hyde\": Generates a hypothetical document to guide the search.\n",
        "#     \"llm_only\": No retrieval at all; asks the LLM directly.\n",
        "\n",
        "# 2. Post-Retrieval Processing\n",
        "# This defines what happens to the documents after they are retrieved but before they are sent to the LLM.\n",
        "# \"post_retrieval_processing\":\n",
        "#     \"none\": No processing; use the documents as-is.\n",
        "#     \"semantic_re_ranking\": Uses a Cross-Encoder to re-rank documents for higher relevance.\n",
        "#     \"contextual_compression\": Uses an LLM to extract only the most relevant sentences from documents.\n",
        "#     \"semantic_re_ranking+contextual_compression\": Applies re-ranking first, then compression for the highest quality context.\n",
        "\n",
        "# 3. Final Prompting Strategy\n",
        "# This determines how the LLM is instructed to use the context to formulate the final answer.\n",
        "# \"prompt_strategy\":\n",
        "#     \"strict_context\": Forbids the LLM from using any knowledge outside of the provided documents.\n",
        "#     \"permissive_context\": Allows the LLM to use its own knowledge to supplement the context.\n",
        "\n",
        "# 4. General Parameters\n",
        "# These are the basic \"knobs\" for any given run.\n",
        "#     \"llm_model\": e.g., \"gpt-4o-mini\", \"gpt-4o\"\n",
        "#     \"index_name\": The specific Pinecone index to target.\n",
        "#     \"namespace\": The namespace within that index.\n",
        "#     \"top_k\": The number of documents to retrieve initially.\n",
        "#     \"reranker_top_n\": The number of documents to return after the semantic re-ranking step.\n",
        "#     \"debug\": True or False."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
